# This file is part of Hypothesis, which may be found at
# https://github.com/HypothesisWorks/hypothesis/
#
# Most of this work is copyright (C) 2013-2020 David R. MacIver
# (david@drmaciver.com), but it contains contributions by others. See
# CONTRIBUTING.rst for a full list of people who may hold copyright, and
# consult the git log if you need to determine who owns an individual
# contribution.
#
# This Source Code Form is subject to the terms of the Mozilla Public License,
# v. 2.0. If a copy of the MPL was not distributed with this file, You can
# obtain one at https://mozilla.org/MPL/2.0/.
#
# END HEADER

import hashlib
import math
from itertools import islice

from hypothesis import HealthCheck, settings
from hypothesis.errors import HypothesisException
from hypothesis.internal.conjecture.data import ConjectureResult, Status
from hypothesis.internal.conjecture.dfa.lstar import LStar
from hypothesis.internal.conjecture.shrinking.learned_dfas import (
    SHRINKING_DFAS,
    __file__ as LEARNED_DFA_FILE,
)

"""
This is a module for learning new DFAs that help normalize test
functions. That is, given a test function that sometimes shrinks
to one thing and sometimes another, this module is designed to
help learn new DFA-based shrink passes that will cause it to
always shrink to the same thing.
"""


class FailedToNormalise(HypothesisException):
    pass


def update_learned_dfas():
    """Write any modifications to the SHRINKING_DFAS dictionary
    back to the learned DFAs file."""

    with open(LEARNED_DFA_FILE) as i:
        source = i.read()

    lines = source.splitlines()

    i = lines.index("# AUTOGENERATED BEGINS")

    del lines[i + 1 :]

    lines.append("")
    lines.append("# fmt: off")
    lines.append("")

    for k, v in sorted(SHRINKING_DFAS.items()):
        lines.append("SHRINKING_DFAS[%r] = %r  # noqa: E501" % (k, v))

    lines.append("")
    lines.append("# fmt: on")

    new_source = "\n".join(lines) + "\n"

    if new_source != source:
        with open(LEARNED_DFA_FILE, "w") as o:
            o.write(new_source)


def find_replacement(u, v, condition):
    """Given u, v with sort_key(u) < sort_key(v) and both satisfying the
    condition, find some substring of v that can be replaced with a shortlex
    smaller one while still satisfying the condition."""

    # We would like to avoid using LStar on large strings as its
    # behaviour can be quadratic or worse. In order to help achieve
    # this we peel off a common prefix and suffix of the two final
    # results and just learn the internal bit where they differ.
    #
    # This potentially reduces the length quite far if there's
    # just one tricky bit of control flow we're struggling to
    # reduce inside a strategy somewhere and the rest of the
    # test function reduces fine.
    common_prefix_length = 0
    while u[common_prefix_length] == v[common_prefix_length]:
        common_prefix_length += 1

    i = 1
    while u[-i] == v[-i]:
        i += 1

    common_suffix_length = i - 1

    ngram_cache = {}

    def ngrams_of_length(k):
        """All strings of length ``k`` appearing in either ``u`` or ``v``."""
        if k == 0:
            return (b"",)
        try:
            return ngram_cache[k]
        except KeyError:
            pass

        return ngram_cache.setdefault(
            k, sorted({t[i : i + k] for t in (u, v) for i in range(len(t) + 1 - k)})
        )

    def suggested_replacements_for(s):
        """Returns suitable candidate replacements for a string. These are
        substrings of ``u`` or ``v`` that are shortlex smaller than ``s`` and
        can reasonably be expected to work in its place."""

        # We skip over strings where replacing them would cause the result to
        # be shorter than ``u``. It's possible that these would work, but we're
        # just trying to reduce ``v`` and don't need to consider them to
        # achieve that.
        min_size = max(0, len(s) - (len(v) - len(u)))
        for k in range(min_size, len(s)):
            yield from ngrams_of_length(k)
        # No branch because we must always break when we hit s in the ngrams
        for t in ngrams_of_length(len(s)): # pragma: no branch
            if t >= s:
                break
            yield t

    for k in range(len(v) + 1 - common_suffix_length - common_prefix_length):
        for i in range(common_prefix_length, len(v) + 1 - common_suffix_length - k):
            print(i, k)
            prefix = v[:i]
            suffix = v[i + k :]
            current = v[i : i + k]
            for replacement in suggested_replacements_for(current):
                if condition(prefix + replacement + suffix):
                    return (prefix, (replacement, current), suffix)
    assert False, "Did not find suitable replacement"  # pragma: no cover


def learn_new_dfas(runner, u, v, predicate, allowed_to_update=True, max_dfas=10):
    """Given two buffers ``u`` and ``v```, learn a DFA that will
    allow the shrinker to normalise them better. ``u`` and ``v``
    should not currently shrink to the same test case when calling
    this function."""
    from hypothesis.internal.conjecture.shrinker import dfa_replacement, sort_key

    assert predicate(runner.cached_test_function(u))
    assert predicate(runner.cached_test_function(v))

    extra_dfas = {}

    prev_dfas = -1
    while True:
        assert len(extra_dfas) > prev_dfas
        prev_dfas = len(extra_dfas)
        u_shrunk = fully_shrink(runner, u, predicate, extra_dfas=extra_dfas)
        v_shrunk = fully_shrink(runner, v, predicate, extra_dfas=extra_dfas)

        u, v = sorted((u_shrunk.buffer, v_shrunk.buffer), key=sort_key)

        if u == v:
            return list(extra_dfas.values())

        assert sort_key(u) < sort_key(v)

        if not allowed_to_update:
            raise FailedToNormalise(
                "Shrinker failed to normalize %r to %r and we are not allowed to learn new DFAs."
                % (u, v)
            )

        if len(extra_dfas) >= max_dfas:
            raise FailedToNormalise(
                (
                    "Test function is too hard to learn. Learned %d DFAs, "
                    "exceeding remaining budget of %d, and still not done."
                )
                % (len(extra_dfas), max_dfas)
            )

        better = runner.cached_test_function(u)
        worse = runner.cached_test_function(v)
        allow_discards = worse.has_discards or better.has_discards

        def is_valid_buffer(buf):
            result = runner.cached_test_function(buf)
            return (
                predicate(result)
                # Because we're often using this to learn strategies
                # rather than entire complex test functions, it's
                # important that our replacements are precise and
                # don't leave the rest of the test case in a weird
                # state.
                and result.buffer == buf
                # Because the shrinker is good at removing discarded
                # data, unless we need discards to allow one or both
                # of u and v to result in valid shrinks, we don't
                # count attempts that have them as valid. This will
                # cause us to match fewer strings, which will make
                # the resulting shrink pass more efficient when run
                # on test functions it wasn't really intended for.
                and (allow_discards or not result.has_discards)
            )

        prefix, (replacement, current), suffix = find_replacement(u, v, is_valid_buffer)
        assert sort_key(replacement) < sort_key(current)

        def is_valid_core(s):
            if not (len(replacement) <= len(s) <= len(current)):
                return False
            return is_valid_buffer(prefix + s + suffix)

        learner = LStar(is_valid_core)

        prev = -1
        while learner.generation != prev:
            prev = learner.generation
            learner.learn(replacement)
            learner.learn(current)

            # L* has a tendency to learn DFAs which wrap around to
            # the beginning. We don't want to it to do that unless
            # it's accurate, so we use these as examples to show
            # check going around the DFA twice.
            learner.learn(replacement * 2)
            learner.learn(current * 2)

            if learner.dfa.max_length(learner.dfa.start) > len(current):
                # The language we learn is finite and bounded above
                # by the length of current. This is important in order
                # to keep our shrink passes reasonably efficient -
                # otherwise they can match far too much. So whenever
                # we learn a DFA that could match a string longer
                # than len(current) we fix it by finding the first
                # string longer than current and learning that as
                # a correction.
                x = next(learner.dfa.all_matching_strings(min_length=len(current) + 1))
                assert not is_valid_core(x)
                learner.learn(x)
                assert not learner.dfa.matches(x)
                assert learner.generation != prev
            else:
                # We mostly care about getting the right answer on the
                # minimal test case, but because we're doing this offline
                # anyway we might as well spend a little more time trying
                # small examples to make sure the learner gets them right.
                for x in islice(learner.dfa.all_matching_strings(), 100):
                    if not is_valid_core(x):
                        learner.learn(x)
                        assert learner.generation != prev
                        break

        # We've now successfully learned a DFA that works for shrinking
        # our failed normalisation further. Canonicalise it into a concrete
        # DFA so we can save it for later.
        new_dfa = learner.dfa.canonicalise()

        assert math.isfinite(new_dfa.max_length(new_dfa.start))

        shrinker = runner.new_shrinker(runner.cached_test_function(v), predicate)

        assert (len(prefix), len(v) - len(suffix)) in shrinker.matching_regions(new_dfa)

        name = "tmp-dfa-" + repr(new_dfa)

        shrinker.extra_dfas[name] = new_dfa

        shrinker.fixate_shrink_passes([dfa_replacement(name)])

        assert sort_key(shrinker.buffer) < sort_key(v)
        extra_dfas[name] = new_dfa


def fully_shrink(runner, test_case, predicate, extra_dfas=None):
    if not isinstance(test_case, ConjectureResult):
        test_case = runner.cached_test_function(test_case)
    while True:
        shrinker = runner.new_shrinker(test_case, predicate)
        if extra_dfas:
            shrinker.extra_dfas.update(extra_dfas)
        shrinker.shrink()
        shrunk = shrinker.shrink_target
        if shrunk.buffer == test_case.buffer:
            break
        test_case = shrunk
    return test_case


def normalize(
    base_name,
    test_function,
    *,
    required_successes=100,
    allowed_to_update=False,
    max_dfas=10
):
    """Attempt to ensure that this test function successfully normalizes - i.e.
    whenever it declares a test case to be interesting, we are able
    to shrink that to the same interesting test case (which logically should
    be the shortlex minimal interesting test case, though we may not be able
    to detect if it is).

    Will run until we have seen ``required_successes`` many interesting test
    cases in a row normalize to the same value.

    If ``allowed_to_update`` is True, whenever we fail to normalize we will
    learn a new DFA-based shrink pass that allows us to make progress. Any
    learned DFAs will be written back into the learned DFA file at the end
    of this function. If ``allowed_to_update`` is False, this will raise an
    error as soon as it encounters a failure to normalize.

    Additionally, if more than ``max_dfas` DFAs are required to normalize
    this test function, this function will raise an error - it's essentially
    designed for small patches that other shrink passes don't cover, and
    if it's learning too many patches then you need a better shrink pass
    than this can provide.
    """
    # Need import inside the function to avoid circular imports
    from hypothesis.internal.conjecture.engine import BUFFER_SIZE, ConjectureRunner

    runner = ConjectureRunner(
        test_function,
        settings=settings(database=None, suppress_health_check=HealthCheck.all()),
        ignore_limits=True,
    )

    seen = set()

    dfas_added = 0

    found_interesting = False
    consecutive_successes = 0
    failures_to_find_interesting = 0
    while consecutive_successes < required_successes:
        attempt = runner.cached_test_function(b"", extend=BUFFER_SIZE)
        if attempt.status < Status.INTERESTING:
            failures_to_find_interesting += 1
            assert (
                found_interesting or failures_to_find_interesting <= 1000
            ), "Test function seems to have no interesting test cases"
            continue

        found_interesting = True

        target = attempt.interesting_origin

        def shrinking_predicate(d):
            return d.status == Status.INTERESTING and d.interesting_origin == target

        if target not in seen:
            seen.add(target)
            runner.shrink(attempt, shrinking_predicate)
            continue

        previous = fully_shrink(
            runner, runner.interesting_examples[target], shrinking_predicate
        )
        current = fully_shrink(runner, attempt, shrinking_predicate)

        dfas = learn_new_dfas(
            runner,
            previous.buffer,
            current.buffer,
            shrinking_predicate,
            allowed_to_update=allowed_to_update,
            max_dfas=max_dfas - dfas_added,
        )

        if not dfas:
            consecutive_successes += 1
            continue

        consecutive_successes = 0
        dfas_added += len(dfas)

        for new_dfa in dfas:
            name = (
                base_name
                + "-"
                + hashlib.sha256(repr(new_dfa).encode("utf-8")).hexdigest()[:10]
            )

            # If there is a name collision this DFA should already be being
            # used for shrinking, so we should have already been able to shrink
            # v further.
            assert name not in SHRINKING_DFAS
            SHRINKING_DFAS[name] = new_dfa

    if dfas_added > 0:
        # We've learned one or more DFAs in the course of normalising, so now
        # we update the file to record those for posterity.
        update_learned_dfas()
